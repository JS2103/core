# Configuration for the prometheus-operator chart.
# Reference: https://github.com/helm/charts/blob/master/stable/prometheus-operator/values.yaml
#
# WARNING: the prometheus-operator chart is complicated and error-prone. If you
# edit this file, run the following command to generate the output with `helm
# template`, and verify that your changes have the expected effect.
#
#   bazel build src/app_charts/prometheus/prometheus-operator-chart.cloud.yaml

nameOverride: kube
fullnameOverride: kube

kubeTargetVersionOverride: "v1.14.10"

# Alertmanagers have to be deployed individually by users.
alertmanager:
  enabled: false

defaultRules:
  rules: # https://github.com/helm/charts/issues/20726
    k8s: false
    kubernetesApps: false
    node: false
    prometheus: false

prometheusOperator:
  # Recent Kubernetes version provide a kube-system/kubelet service by default.
  kubeletService:
    enabled: false

prometheus:
  prometheusSpec:
    # Pick up all service monitors across all namespaces.
    serviceMonitorNamespaceSelector:
      any: true
    serviceMonitorSelector:
      any: true
    # Pick up all pod monitors across all namespaces.
    podMonitorNamespaceSelector:
      any: true
    podMonitorSelector:
      any: true
    ruleNamespaceSelector:
      any: true
    externalUrl: "https://${CLOUD_ROBOTICS_DOMAIN}/prometheus/"
    retention: "120d"
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ssd
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              # At the time of writing this, Prometheus ingests about 1500
              # samples/sec, which should result it ~17GB for the 120 days of
              # retention.
              # If higher ingestion throughput is observed, this should be
              # adjusted linearly.
              storage: 60Gi
    # Pick up user-created Alertmanager pods with app=alertmanager and a non-empty port.
    additionalAlertManagerConfigs:
    - kubernetes_sd_configs:
        - role: service
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_label_app]
        regex: kube-alertmanager
        action: keep

# etcd, scheduler, and controller-manager are managed by GKE and hidden.
kubeEtcd:
  enabled: false
kubeControllerManager:
  enabled: false
kubeScheduler:
  enabled: false
coreDns:
  enabled: false
# We provide our own ServiceMonitor for kubelets since the standard one doesn't work out
# of the box on GKE.
kubelet:
  enabled: false

# Throws an invalid namespace "kube-system" error during deployment
kubeProxy:
  enabled: false

# Subcharts

grafana:
  env:
    GF_SERVER_DOMAIN: "${CLOUD_ROBOTICS_DOMAIN}"
    GF_SERVER_ROOT_URL: "https://${CLOUD_ROBOTICS_DOMAIN}/grafana"
    GF_AUTH_ANONYMOUS_ENABLED: "true"
  # Load dashboards from configmaps with a given label across all namespaces.
  sidecar:
    dashboards:
      enabled: true
      label: grafana # Label our own legacy grafana-operator uses.
      searchNamespace: ALL
  grafana.ini:
    analytics:
      check_for_updates: false
